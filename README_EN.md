[**ä¸­æ–‡**](./README.md) | [**English**](./README_EN.md) 

<p align="center">
    <br>
    <img src="./pics/promptCBLUE_en_banner_v0.png" width="600"/>
    <br>
</p>
<p align="center">
    <img alt="GitHub" src="https://img.shields.io/github/license/ymcui/Chinese-LLaMA-Alpaca.svg?color=blue&style=flat-square">
    <img alt="GitHub top language" src="https://img.shields.io/github/languages/top/ymcui/Chinese-LLaMA-Alpaca">
</p>


The emergence of large language models (LLMs) represented by ChatGPT and GPT-4 has sparked a new wave of research in the field of natural language processing, demonstrating capabilities similar to those of general artificial intelligence (AGI) and attracting widespread attention from the industry. With the prevalence of LLMs, almost all NLP tasks have been transformed into prompt-based language generation tasks. However, in the Chinese medical NLP community, there is still no unified task-based benchmark for evaluation.

ä¸ºæ¨åŠ¨LLMåœ¨åŒ»ç–—é¢†åŸŸçš„å‘å±•å’Œè½åœ°ï¼Œåä¸œå¸ˆèŒƒå¤§å­¦ç‹æ™“ç²æ•™æˆå›¢é˜Ÿè”åˆé˜¿é‡Œå·´å·´å¤©æ± å¹³å°ï¼Œå¤æ—¦å¤§å­¦ï¼Œå¤æ—¦å¤§å­¦é™„å±åå±±åŒ»é™¢ï¼Œä¸œåŒ—å¤§å­¦ï¼Œå“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ï¼ˆæ·±åœ³ï¼‰ï¼Œé¹åŸå®éªŒå®¤ä¸åŒæµå¤§å­¦æ¨å‡º**PromptCBLUE**è¯„æµ‹åŸºå‡†, å¯¹[CBLUE](https://tianchi.aliyun.com/dataset/95414)åŸºå‡†è¿›è¡ŒäºŒæ¬¡å¼€å‘ï¼Œå°†16ç§ä¸åŒçš„åŒ»ç–—åœºæ™¯NLPä»»åŠ¡å…¨éƒ¨è½¬åŒ–ä¸ºåŸºäºæç¤ºçš„è¯­è¨€ç”Ÿæˆä»»åŠ¡,å½¢æˆé¦–ä¸ªä¸­æ–‡åŒ»ç–—åœºæ™¯çš„LLMè¯„æµ‹åŸºå‡†ã€‚**PromptCBLUE**ä½œä¸ºCCKS-2023çš„è¯„æµ‹ä»»åŠ¡ä¹‹ä¸€ï¼Œå·²åœ¨é˜¿é‡Œå·´å·´å¤©æ± å¤§èµ›å¹³å°ä¸Šçº¿è¿›è¡Œå¼€æ”¾è¯„æµ‹ï¼Œæ¬¢è¿å„ä½å¸ˆç”ŸæŠ¥åå‚èµ›(åˆ·æ¦œ)ã€‚

To promote the developments and applications of LLMs in the medical field, Professor Xiaoling Wang's team from East China Normal University, in collaboration with Alibaba Tianchi Platform, Fudan University, Huashan Hospital affiliated to Fudan University, Northeastern University, Harbin Institute of Technology (Shenzhen), Peng Cheng Laboratory, and Tongji University, has launched the **PromptCBLUE** evaluation benchmark by modifying the [CBLUE](https://tianchi.aliyun.com/dataset/95414) benchmark. This benchmark has converted all 16 different medical NLP tasks into prompt-based language generation tasks, creating the first Chinese medical LLM evaluation benchmark. PromptCBLUE is one of the evaluation tasks for the [CCKS-2023](https://sigkg.cn/ccks2023/evaluation) conference and has been launched for open evaluation on the Alibaba Tianchi competition platform. Industrial practitioners, students and researchers are welcome to register and participate in the competition.

In consideration of the potential involvement of commercial data in LLM training and the limitations posed by various external factors on the open-sourcing of large language models, we have opened two tracks for the PromptCBLUE evaluation:
- General track: This track accepts evaluations from enterprises, universities, open-source communities, research teams, or individuals who have developed their own LLMs. Participants are not required to open-source their models. The evaluation website for this track is available at [CCKS2023-PromptCBLUE General Track](https://tianchi.aliyun.com/competition/entrance/532085/introduction).
- Open-source track: This track is open to all participating teams who must use an open-source large-scale model framework and only train/fine-tune using open-source datasets or datasets that can be submitted to the competition organizer for review. The evaluation website for this track is available at [CCKS2023-PromptCBLUE Open-source Track](https://tianchi.aliyun.com/competition/entrance/532084/introduction).


To assist in the enhancement of LLM's abilities in the medical field, we are open-sourcing the following data/model resources:
- ğŸš€ [ChatMed_Consult_Dataset](https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset)ï¼šA Chinese medical online consultation dataset containing 500k+ online consultation queries and responses made by ChatGPT.
- ğŸš€ [ChatMed-Consult model](https://huggingface.co/michaelwzhu/ChatMed-Consult): A large Chinese medical consultation model fine-tuned on [ChatMed_Consult_Dataset](https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset). The model is based on the [LlaMA-7b](https://github.com/facebookresearch/llama) merged with LoRA weights from [Chinese-LlaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca). All data and code are publicly available at [ChatMedé¡¹ç›®](https://github.com/michael-wzhu/ChatMed).
- â³ [ChatMed_TCM_Dataset](https://huggingface.co/datasets/michaelwzhu/ChatMed_TCM_Dataset). A dataset of instructions for traditional Chinese medicine (TCM) with 26k+ samples generated using the [entity-centric self-instruct method](https://github.com/michael-wzhu/ChatMed/blob/main/src/) based on our open-sourced [TCM knowledge graph](https://github.com/ywjawmw/TCM_KG) and ChatGPT responses.
- 
- 
- ä»¥æˆ‘ä»¬å¼€æºçš„[ä¸­åŒ»è¯çŸ¥è¯†å›¾è°±](https://github.com/ywjawmw/TCM_KG)ä¸ºåŸºç¡€ï¼Œé‡‡ç”¨[ä»¥å®ä½“ä¸ºä¸­å¿ƒçš„è‡ªæŒ‡ä»¤æ–¹æ³•(entity-centric self-instruct)](https://github.com/michael-wzhu/ChatMed/blob/main/src/)ï¼Œè°ƒç”¨ChatGPTå¾—åˆ°2.6w+çš„å›´ç»•ä¸­åŒ»è¯çš„æŒ‡ä»¤æ•°æ®ã€‚
- â³ [ä¸­åŒ»è¯å¤§æ¨¡å‹ChatMed-TCM](https://huggingface.co/michaelwzhu/ChatMed-TCM) : å¤§æ¨¡å‹èµ‹èƒ½ä¸­åŒ»è¯ä¼ æ‰¿ã€‚è¿™ä¸€æ¨¡å‹çš„è®­ç»ƒæ•°æ®ä¸º[ä¸­åŒ»è¯æŒ‡ä»¤æ•°æ®é›†ChatMed_TCM_Dataset](https://huggingface.co/datasets/michaelwzhu/ChatMed_TCM_Dataset)ã€‚ChatMed-TCMæ¨¡å‹ä¹Ÿæ˜¯ä»¥LlaMAä¸ºåº•åº§ï¼Œé‡‡ç”¨LoRAå¾®è°ƒå¾—åˆ°ã€‚

â³ ChatMed_TCM_Dataset: A dataset of Chinese medicine instructions with 26k+ samples generated using the entity-centric self-instruct method based on our open-sourced Chinese medicine knowledge graph and ChatGPT.
â³ ChatMed-TCM model: A large Chinese medicine model based on LlaMA and fine-tuned on the ChatMed_TCM_Dataset for Chinese medicine inheritance. The model also incorporates LoRA for efficient parameter fine-tuning.


----

[Text2DT](https://github.com/michael-wzhu/Text2DT_Baseline) | [ä¸­æ–‡åŒ»ç–—åœ¨çº¿é—®è¯Šæ•°æ®é›†ChatMed_Consult_Dataset](https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset) | [ä¸­æ–‡é—®è¯Šå¤§æ¨¡å‹ChatMed-Consult](https://huggingface.co/michaelwzhu/ChatMed-Consult) | [ä¸­åŒ»è¯æŒ‡ä»¤æ•°æ®é›†ChatMed_TCM_Dataset](https://huggingface.co/datasets/michaelwzhu/ChatMed_TCM_Dataset) |  [ä¸­åŒ»è¯å¤§æ¨¡å‹ChatMed-TCM](https://huggingface.co/michaelwzhu/ChatMed-TCM) 


## æ›´æ–°